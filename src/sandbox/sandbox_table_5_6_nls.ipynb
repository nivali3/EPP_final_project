{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DellaVigna and Pope, 2018, \"What Motivates Effort? Evidence and Expert Forecasts\", Tables 5 and 6, NLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors:  \n",
    "\n",
    "- Massimiliano Pozzi (Bocconi University, pozzi.massimiliano@studbocconi.it)\n",
    "- Salvatore Nunnari (Bocconi University, salvatore.nunnari@unibocconi.it)\n",
    "\n",
    "#### Description:\n",
    "\n",
    "The code in this Jupyter notebook replicates columns 2 and 4 in Panel A of Table 5; columns 3 and 6 in Panel B of Table 5; and Panel A of Table 6. The estimates in these panels and columns are derived with Non-Linear-Least-Squares.\n",
    "\n",
    "This notebook was tested with the following packages versions:\n",
    "- Pozzi:   (Anaconda 4.10.3 on Windows 10 Pro) : python 3.8.3, numpy 1.18.5, pandas 1.0.5, sklearn 1.0\n",
    "- Nunnari: (Anaconda 4.10.1 on macOS 10.15.7): python 3.8.10, numpy 1.20.2, pandas 1.2.4, scipy 1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   scipy.stats import norm\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Data Preparation\n",
    "\n",
    "We import the relevant dataset containing data on the number of buttonpresses in the different treatments and for different piece rates wage that the participants received when completing the task. We then create a series of variables that are needed for estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "\n",
    "dt = pd.read_stata('../input/mturk_clean_data_short.dta')\n",
    "\n",
    "# Create new variables needed for estimation:\n",
    "\n",
    "# Create piece-rate payoffs per 100 button presses (p)\n",
    "\n",
    "dt['payoff_per_100'] = 0\n",
    "dt.loc[dt.treatment == '1.1', 'payoff_per_100'] = 0.01\n",
    "dt.loc[dt.treatment == '1.2', 'payoff_per_100'] = 0.1\n",
    "dt.loc[dt.treatment == '1.3', 'payoff_per_100'] = 0.0\n",
    "dt.loc[dt.treatment == '2'  , 'payoff_per_100'] = 0.001\n",
    "dt.loc[dt.treatment == '1.4', 'payoff_per_100'] = 0.04\n",
    "dt.loc[dt.treatment == '4.1', 'payoff_per_100'] = 0.01\n",
    "dt.loc[dt.treatment == '4.2', 'payoff_per_100'] = 0.01\n",
    "dt.loc[dt.treatment == '6.2', 'payoff_per_100'] = 0.02\n",
    "dt.loc[dt.treatment == '6.1', 'payoff_per_100'] = 1\n",
    "\n",
    "# (alpha/a) create payoff per 100 to charity and dummy charity\n",
    "\n",
    "dt['payoff_charity_per_100'] = 0\n",
    "dt.loc[dt.treatment == '3.1', 'payoff_charity_per_100'] = 0.01\n",
    "dt.loc[dt.treatment == '3.2', 'payoff_charity_per_100'] = 0.1\n",
    "dt['dummy_charity'] = 0\n",
    "dt.loc[dt.treatment == '3.1', 'dummy_charity'] = 1\n",
    "dt.loc[dt.treatment == '3.2', 'dummy_charity'] = 1\n",
    "\n",
    "# (beta/delta) create payoff per 100 delayed by 2 weeks and dummy delay\n",
    "\n",
    "dt['delay_wks'] = 0\n",
    "dt.loc[dt.treatment == '4.1', 'delay_wks'] = 2\n",
    "dt.loc[dt.treatment == '4.2', 'delay_wks'] = 4\n",
    "dt['delay_dummy'] = 0\n",
    "dt.loc[dt.treatment == '4.1', 'delay_dummy'] = 1\n",
    "dt.loc[dt.treatment == '4.2', 'delay_dummy'] = 1\n",
    "\n",
    "# probability weights to back out curvature and dummy\n",
    "\n",
    "dt['prob'] = 1\n",
    "dt.loc[dt.treatment == '6.2', 'prob'] = 0.5\n",
    "dt.loc[dt.treatment == '6.1', 'prob'] = 0.01\n",
    "dt['weight_dummy'] = 0\n",
    "dt.loc[dt.treatment == '6.1', 'weight_dummy'] = 1\n",
    "\n",
    "# dummy for gift exchange\n",
    "\n",
    "dt['gift_dummy'] = 0\n",
    "dt.loc[dt.treatment == '10', 'gift_dummy'] = 1\n",
    "\n",
    "# generating effort and log effort. authors round buttonpressed to nearest 100 value. If 0 set it to 25.\n",
    "\n",
    "dt['buttonpresses'] = dt['buttonpresses'] + 0.1 # python rounds 50 to 0, while stata to 100. by adding a small value we avoid this mismatch\n",
    "dt['buttonpresses_nearest_100'] = round(dt['buttonpresses'],-2)\n",
    "dt.loc[dt.buttonpresses_nearest_100 == 0, 'buttonpresses_nearest_100'] = 25\n",
    "dt['logbuttonpresses_nearest_100']  = np.log(dt['buttonpresses_nearest_100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Estimation Technique (Section 2 in the Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is one of costly effort, where an agent needs to choose the optimal effort (in this case the number of buttons pressed in a 10 minute session) to solve a simple tradeoff problem between disutility of effort and consumption utility derived from the consequent payment. On top of this simple problem, the authors use 18 different treatments to examine the effects of standard monetary incentives, behavioral factors (e.g., social preferences and reference dependence) and non-monetary incentives. We briefly examine here the benchmark model and the solutions found when using non-linear-least-squares.\n",
    "\n",
    "The model for treatment 1.1, 1.2 and 1.3 can be written as follows:\n",
    "\n",
    "$$ \\max_{e\\geq0} \\;\\; (s+p)e-c(e) $$\n",
    "\n",
    "Where e is the number of buttons pressed, p is the piece-rate that varies across treatments, s is a parameter that captures intrinsic motivation, and c(e) is an heterogeneous convex cost function, either of power or exponential form:\n",
    "\n",
    "$$ c(e)=\\frac{ke^{1+\\gamma}}{1+\\gamma}exp(-\\gamma \\epsilon_j) \\qquad \\qquad c(e)=\\frac{kexp(\\gamma e)}{\\gamma}exp(-\\gamma \\epsilon_j)$$\n",
    "\n",
    "The variable &epsilon;<sub>j</sub> is normally distributed, &epsilon;<sub>j</sub>~N(0,&sigma;<sub>j</sub>), so that the additional noise term exp(-&gamma;&epsilon;<sub>j</sub>) has a lognormal distribution. The first order condition implied by the maximization problem after taking logs is the following:\n",
    "\n",
    "$$ log(e_j)=\\frac{1}{\\gamma}[log(s+p)-log(k)]+\\epsilon_j \\qquad \\qquad e_j=\\frac{1}{\\gamma}[log(s+p)-log(k)]+\\epsilon_j $$ \n",
    "\n",
    "where the first equation assumes a power cost function and the second equation assumes an exponential cost function. By using non-linear-least-squares, our goal is to minimize the sum of squared distances between the observed effort and the optimal effort computed above, namely:\n",
    "\n",
    "$$ \\min \\sum_{j=1}^J(y_j-f(x_j,\\theta))^2 $$\n",
    "\n",
    "where j is a generic individual observation, y is the observed effort, and f(x,&theta;) is the function which computes the optimal effort (the first order condition) depending on the data and a set of parameters &theta;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "### Point Estimates and Standard Errors\n",
    "\n",
    "We now compute the NLS estimates for Tables 5 and 6. Since there are many different specifications (5 columns for the power cost function and 5 for the exponential cost function), we preferred to write each function to compute f(x,&theta;) separately instead of writing a single function with many if statements. Hopefully, this will make each specification clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate procedure for s, k, gamma in benchmark case with exp cost function\n",
    "\n",
    "# Define the benchmark sample by creating dummies equal to one if in treatment 1.1, 1.2, 1.3 \n",
    "\n",
    "dt['t1.1']= (dt['treatment']=='1.1').astype(int)\n",
    "dt['t1.2']= (dt['treatment']=='1.2').astype(int)\n",
    "dt['t1.3']= (dt['treatment']=='1.3').astype(int)\n",
    "dt['dummy1']= dt['t1.1']+dt['t1.2']+dt['t1.3']\n",
    "\n",
    "# Set the initial values for the optimization procedure and scalers for k and s in the exp cost function case\n",
    "\n",
    "gamma_init_exp, k_init_exp, s_init_exp =  0.015645717, 1.69443, 3.69198\n",
    "st_values_exp = [gamma_init_exp, k_init_exp, s_init_exp]\n",
    "k_scaler_exp, s_scaler_exp = 1e+16, 1e+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that computes the optimal effort, what we called f(x,θ) above\n",
    "# pay100 is the column we created containing the piece rate for different treatments\n",
    "# g, k, s are the parameters to estimate (our θ vector). g stands for gamma.\n",
    "\n",
    "def benchmarkExp(pay100, g, k, s):\n",
    "    \n",
    "    check1 = k/k_scaler_exp            # 'first'  component to compute f(x,θ). We call it check1 since it will enter a log, so we need to be careful with its value being > 0\n",
    "    check2 = s/s_scaler_exp + pay100   # 'second' component to compute f(x,θ)\n",
    "    \n",
    "    f_x = (-1/g * np.log(check1) +1/g * np.log(check2))   # f(x,θ) written above\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solution to the problem by non-linear least squares \n",
    "\n",
    "sol = opt.curve_fit(benchmarkExp,\n",
    "                    dt.loc[dt['dummy1']==1].payoff_per_100,\n",
    "                    dt.loc[dt['dummy1']==1].buttonpresses_nearest_100,\n",
    "                    st_values_exp)\n",
    "\n",
    "be54 = sol[0]                        # sol[0] is the array containing our estimates\n",
    "se54 = np.sqrt(np.diagonal(sol[1]))  # sol[1] is a 3x3 variance-covariance matrix of our estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate procedure for s, k, gamma in benchmark case with power cost function\n",
    "\n",
    "gamma_init_power, k_init_power, s_init_power =  19.8117987, 1.66306e-10, 7.74996\n",
    "st_values_power = [gamma_init_power, k_init_power, s_init_power]\n",
    "k_scaler_power, s_scaler_power = 1e+57,1e+6\n",
    "\n",
    "# Define f(x,θ) in the power case\n",
    "\n",
    "def benchmarkPower(pay100, g, k, s):\n",
    "    \n",
    "    check1= max(k/k_scaler_power, 1e-115)                  # since check1 will enter log it must be greater than zero\n",
    "    check2= np.maximum(s/s_scaler_power + pay100, 1e-10)   # np.maximum computes the max element wise. We do not want a negative value inside log\n",
    "    \n",
    "    f_x = (-1/g * np.log(check1) +1/g * np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solution to the problem by non-linear least squares. \n",
    "# We find some differences with respect to the results found by the authors in the case of the power cost function. Even by \n",
    "# changing the initial guesses or minimization algorithm we still end up with slightly different results.\n",
    "\n",
    "sol = opt.curve_fit(benchmarkPower,\n",
    "                    dt.loc[dt['dummy1']==1].payoff_per_100,\n",
    "                    dt.loc[dt['dummy1']==1].logbuttonpresses_nearest_100,\n",
    "                    st_values_power)\n",
    "bp52 = sol[0]                       # sol[0] is the array containing our estimates\n",
    "sp52 = np.sqrt(np.diagonal(sol[1])) # sol[1] is a 3x3 variance-covariance matrix of our estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try using a different package to find the NLS estimates to see if we get closer to the authors' but without success. opt.least_squares\n",
    "# takes as input directly the squared residuals, so we need to specify a different objective function. \n",
    "\n",
    "def benchmarkPower_least_squares(params):\n",
    "    \n",
    "    pay100 = np.array(dt.loc[dt['dummy1']==1].payoff_per_100)\n",
    "    logbuttonpresses = np.array(dt.loc[dt['dummy1']==1].logbuttonpresses_nearest_100)\n",
    "    g, k, s = params\n",
    "    \n",
    "    check1= max(k/k_scaler_power, 1e-115)\n",
    "    check2= np.maximum(s/s_scaler_power + pay100, 1e-10)   \n",
    "    \n",
    "    f_x = 0.5*((-1/g * np.log(check1) +1/g * np.log(check2))-logbuttonpresses)**2\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "sol_least_square = opt.least_squares(benchmarkPower_least_squares,\n",
    "                        st_values_power,\n",
    "                        xtol=1e-15,\n",
    "                        ftol=1e-15,\n",
    "                        gtol=1e-15,\n",
    "                        method='lm')\n",
    "bp52_least_square = sol_least_square.x  # sol.x is the array containing our estimates\n",
    "\n",
    "# We tried minimizing the objective function also using a general framework and not a package specific for non-linear-least-square\n",
    "# When using opt.minimize we need to use as input directly the function to minimize, in this case the sum of squared residuals\n",
    "\n",
    "def benchmarkPower_opt(params):\n",
    "    \n",
    "    pay100 = np.array(dt.loc[dt['dummy1']==1].payoff_per_100)\n",
    "    logbuttonpresses = np.array(dt.loc[dt['dummy1']==1].logbuttonpresses_nearest_100)\n",
    "    g, k, s = params\n",
    "    \n",
    "    check1= max(k/k_scaler_power, 1e-115)\n",
    "    check2= np.maximum(s/s_scaler_power + pay100, 1e-10)   \n",
    "    \n",
    "    f_x = np.sum(0.5*((-1/g * np.log(check1) +1/g * np.log(check2))-logbuttonpresses)**2)\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "sol_opt = opt.minimize(benchmarkPower_opt,\n",
    "                       st_values_power,\n",
    "                       method='Nelder-Mead',\n",
    "                       options={'maxiter': 2500})\n",
    "bp52_opt = sol_opt.x\n",
    "\n",
    "# We create a table and show the results we obtained\n",
    "\n",
    "from IPython.display import display\n",
    "pn = [\"Curvature γ of cost function\",\"Level k of cost of effort\", \"Intrinsic motivation s\",\"Min obj. function\"]\n",
    "bp52_aut = [20.546,5.12e-13,3.17]\n",
    "r1 = pd.DataFrame({'parameters':pn,'curve_fit':np.round([*bp52,2*benchmarkPower_opt(bp52)],3),\n",
    "                   'least_square':np.round([*bp52_least_square,2*benchmarkPower_opt(bp52_least_square)],3),\n",
    "                   'minimize_nd':np.round([*bp52_opt,2*benchmarkPower_opt(bp52_opt)],3),\n",
    "                   'authors':np.round([*bp52_aut,2*benchmarkPower_opt(bp52_aut)],3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain estimates with different minimization algorithms implemented by different functions available in the scipy package. Note that the estimates for k and s are very small in absolute value: in the table below, the the estimates of k must be divided by 1e+57 and the estimates for s by 1e+6. We also show the authors' estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>curve_fit</th>\n",
       "      <th>least_square</th>\n",
       "      <th>minimize_nd</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Curvature γ of cost function</td>\n",
       "      <td>21.194</td>\n",
       "      <td>21.787</td>\n",
       "      <td>21.266</td>\n",
       "      <td>20.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Level k of cost of effort</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intrinsic motivation s</td>\n",
       "      <td>1.377</td>\n",
       "      <td>0.096</td>\n",
       "      <td>1.331</td>\n",
       "      <td>3.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Min obj. function</td>\n",
       "      <td>670.610</td>\n",
       "      <td>1112.339</td>\n",
       "      <td>670.610</td>\n",
       "      <td>672.387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     parameters  curve_fit  least_square  minimize_nd  authors\n",
       "0  Curvature γ of cost function     21.194        21.787       21.266   20.546\n",
       "1     Level k of cost of effort      0.000         0.000        0.000    0.000\n",
       "2        Intrinsic motivation s      1.377         0.096        1.331    3.170\n",
       "3             Min obj. function    670.610      1112.339      670.610  672.387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the least_square function performs the worst, while the curve_fit function (which uses the Levenberg-Marquardt minimization algorithm) and the minimize function (which uses the Nelder-Mead minimization algorithm) return a  similar value for the objective function but slightly different estimates for the parameters. Since, in this case, different minimization algorithms implemented with the same programming language (python) result in different estimates and/or values of the objective function, it is not surprising that there are small discrepancies between our estimates and the authors' estimates (the authors use the Gauss-Newton minimization algorithm implemented in Stata). At the same time, the differences are small in absolute value (and limited to the NLSS estimation method, there are no discrepancies when using GMM) and the estimated values of k and s are always statistically indistinguishable from 0. More importantly, the economic implications of the estimated parameters and the qualitative conclusions on what motivates effort in the experiment are unaffected by the choice of programming language and minimization algorithm. Below, we report the results we obtained with the curve_fit function since this also returns an estimate for the variance-covariance matrix for the parameters. \n",
    "\n",
    "Next, we replicate Panel B of Table 5, where we estimate all parameters of interest without the weight on probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allnoweight Exp. Create dummies for this specification\n",
    "\n",
    "dt['t3.1']= (dt['treatment']=='3.1').astype(int)\n",
    "dt['t3.2']= (dt['treatment']=='3.2').astype(int)\n",
    "dt['t4.1']= (dt['treatment']=='4.1').astype(int)\n",
    "dt['t4.2']= (dt['treatment']=='4.2').astype(int)\n",
    "dt['t10'] = (dt['treatment']=='10').astype(int)\n",
    "dt['samplenw']= dt['dummy1']+dt['t3.1']+dt['t3.2']+dt['t4.1']+dt['t4.2']+dt['t10']\n",
    "\n",
    "# Define the initial guesses for the exponential cost function case\n",
    "\n",
    "alpha_init, a_init, beta_init, delta_init, gift_init = 0.003, 0.13, 1.16, 0.75, 5e-6\n",
    "stvale_spec = [alpha_init, a_init, gift_init, beta_init, delta_init]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the f(x,θ) to estimate all parameters but the probability weight in the exp case\n",
    "\n",
    "# xdata is the vector containing the explanatory variables:\n",
    "\n",
    "# gd is gift dummy\n",
    "# dd is delay dummy\n",
    "# dw is delay weeks\n",
    "# paychar is pay in charity treatment\n",
    "# dc is dummy charity\n",
    "\n",
    "# parameters:\n",
    "\n",
    "# g, k, s are the same parameters from before\n",
    "# alpha is the pure altruism coefficient\n",
    "# a is the warm glow coefficient\n",
    "# gift is the gift exchange coefficient Δs\n",
    "# beta is the present bias paramater\n",
    "# delta is the (weekly) discount factor\n",
    "\n",
    "def noweightExp(xdata, g, k, s, alpha, a, gift, beta, delta):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    gd = xdata[1]\n",
    "    dd = xdata[2]\n",
    "    dw = xdata[3]\n",
    "    paychar = xdata[4]\n",
    "    dc = xdata[5]\n",
    "    \n",
    "    check1 = k/k_scaler_exp\n",
    "    check2 = s/s_scaler_exp + gift*0.4*gd + (beta**dd)*(delta**dw)*pay100 + alpha*paychar +a*0.01*dc\n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solution to the problem by non-linear least squares \n",
    "\n",
    "st_valuesnoweight_exp = np.concatenate((st_values_exp,stvale_spec)) # starting values\n",
    "\n",
    "args = [dt.loc[dt['samplenw']==1].payoff_per_100, dt.loc[dt['samplenw']==1].gift_dummy, dt.loc[dt['samplenw']==1].delay_dummy,\n",
    "        dt.loc[dt['samplenw']==1].delay_wks, dt.loc[dt['samplenw']==1].payoff_charity_per_100, dt.loc[dt['samplenw']==1].dummy_charity]\n",
    "\n",
    "sol = opt.curve_fit(noweightExp, \n",
    "                    args,\n",
    "                    dt.loc[dt['samplenw']==1].buttonpresses_nearest_100,\n",
    "                    st_valuesnoweight_exp)\n",
    "be56 = sol[0]\n",
    "se56 = np.sqrt(np.diagonal(sol[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the f(x,θ) to estimate all parameters but the probability weight in the power case\n",
    "\n",
    "def noweightPower(xdata, g, k, s, alpha, a, gift, beta, delta):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    gd = xdata[1]\n",
    "    dd = xdata[2]\n",
    "    dw = xdata[3]\n",
    "    paychar = xdata[4]\n",
    "    dc = xdata[5]\n",
    "    \n",
    "    check1= max(k/k_scaler_power, 1e-115)\n",
    "    check2= np.maximum(s/s_scaler_power + gift*0.4*gd + (beta**dd)*(delta**dw)*pay100 + alpha*paychar + a*0.01*dc, 1e-10)  \n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solution to the problem by non-linear least squares \n",
    "\n",
    "st_valuesnoweight_power = np.concatenate((st_values_power,stvale_spec)) # starting values\n",
    "\n",
    "sol = opt.curve_fit(noweightPower, \n",
    "                    args,\n",
    "                    dt.loc[dt['samplenw']==1].logbuttonpresses_nearest_100,\n",
    "                    st_valuesnoweight_power)\n",
    "bp53 = sol[0] \n",
    "sp53 = np.sqrt(np.diagonal(sol[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 5: non-linear-least-squares estimates of behavioural parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>power_est</th>\n",
       "      <th>power_se</th>\n",
       "      <th>exp_est</th>\n",
       "      <th>exp_se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Curvature γ of cost function</td>\n",
       "      <td>21.194</td>\n",
       "      <td>7.3990</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.00415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Level k of cost of effort</td>\n",
       "      <td>5.95e-72</td>\n",
       "      <td>3.33e-70</td>\n",
       "      <td>1.71e-16</td>\n",
       "      <td>1.49e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intrinsic motivation s</td>\n",
       "      <td>1.38e-6</td>\n",
       "      <td>4.93e-6</td>\n",
       "      <td>3.72e-6</td>\n",
       "      <td>9.16e-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social preferences α</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Warm glow coefficient a</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>0.2869</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gift exchange Δs</td>\n",
       "      <td>3.17e-5</td>\n",
       "      <td>8.00e-5</td>\n",
       "      <td>2.35e-5</td>\n",
       "      <td>4.82e-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Present bias β</td>\n",
       "      <td>1.6123</td>\n",
       "      <td>2.055</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Weekly) discount factor δ</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2923</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     parameters power_est  power_se   exp_est    exp_se\n",
       "0  Curvature γ of cost function    21.194    7.3990    0.0156   0.00415\n",
       "1     Level k of cost of effort  5.95e-72  3.33e-70  1.71e-16  1.49e-15\n",
       "2        Intrinsic motivation s   1.38e-6   4.93e-6   3.72e-6   9.16e-6\n",
       "3          Social preferences α    0.0132    0.0295     0.004     0.011\n",
       "4       Warm glow coefficient a    0.2628    0.2869     0.143     0.143\n",
       "5              Gift exchange Δs   3.17e-5   8.00e-5   2.35e-5   4.82e-5\n",
       "6                Present bias β    1.6123     2.055      1.24       1.3\n",
       "7    (Weekly) discount factor δ      0.75    0.2923      0.75      0.24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and save the dataframe for table 5 NLS estimates. We leave standard errors for all parameters instead of confidence intervals for some.\n",
    "# Point estimates for power case do not coincide precisely as explained above. Standard errors do not coincide precisely because of \n",
    "# the differences in the point estimates and because we leave here non-robust standard errors provided by curve_fit. To see an implementation of \n",
    "# the formula for robust standard errors please refer to the python or julia notebooks for table_1 of augenblick-rabin or table_1 of bruhin-fehr-schunk.\n",
    "# The formula is the same as in the cited notebooks without considering the clustering at the individual level.\n",
    "\n",
    "from decimal import Decimal\n",
    "\n",
    "params_name = [\"Curvature γ of cost function\", \"Level k of cost of effort\", \"Intrinsic motivation s\",\"Social preferences α\",\n",
    "                \"Warm glow coefficient a\",\"Gift exchange Δs\", \"Present bias β\",\"(Weekly) discount factor δ\"]\n",
    "\n",
    "be5 = ['{0:.3}'.format(Decimal(be54[0])), '{0:.2e}'.format(Decimal(be54[1]/1e+16)), '{0:.2e}'.format(Decimal(be54[2]/1e+6)),\n",
    "       round(be56[3],3), round(be56[4],3), '{0:.2e}'.format(Decimal(be56[5])), round(be56[6],2), round(be56[7],2)]\n",
    "se5 = ['{0:.3}'.format(Decimal(se54[0])), '{0:.2e}'.format(Decimal(se54[1]/1e+16)), '{0:.2e}'.format(Decimal(se54[2]/1e+6)),\n",
    "       round(se56[3],3), round(se56[4],3), '{0:.2e}'.format(Decimal(se56[5])), round(se56[6],2), round(se56[7],2)]\n",
    "\n",
    "bp5 = ['{0:.5}'.format(Decimal(bp52[0])), '{0:.2e}'.format(Decimal(bp52[1]/1e+57)), '{0:.2e}'.format(Decimal(bp52[2]/1e+6)),\n",
    "       round(bp53[3],4), round(bp53[4],4), '{0:.2e}'.format(Decimal(bp53[5])), round(bp53[6],4), round(bp53[7],4)]\n",
    "sp5 = ['{0:.5}'.format(Decimal(sp52[0])), '{0:.2e}'.format(Decimal(sp52[1]/1e+57)), '{0:.2e}'.format(Decimal(sp52[2]/1e+6)),\n",
    "       round(sp53[3],4), round(sp53[4],4), '{0:.2e}'.format(Decimal(sp53[5])), round(sp53[6],4), round(sp53[7],4)]\n",
    "\n",
    "t5 = pd.DataFrame({'parameters':params_name,'power_est':bp5,'power_se':sp5,'exp_est':be5,'exp_se':se5})\n",
    "t5.to_csv('../output/table5NLS_python.csv')\n",
    "\n",
    "print('Table 5: non-linear-least-squares estimates of behavioural parameters')\n",
    "display(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of squared errors using our estimates is: 1539.446\n",
      "The sum of squared errors using the authors'estimates is: 1543.057\n",
      "The small difference between the Stata sse and the sse computed by us are most likely due to rounding.\n"
     ]
    }
   ],
   "source": [
    "# Check for possible mistakes in the power case since authors' estimates are different from ours.\n",
    "# We compare the sum of squared errors using our estimates and the authors'.\n",
    "# By running the \"1_NLS_main.do\" do-file provided in the replication code they obtain an sse = 1542.141\n",
    "\n",
    "# define the function that computes the sse\n",
    "\n",
    "def noweight_sse(xdata, g, k, s, alpha, a, gift, beta, delta):\n",
    "\n",
    "    pay100 = xdata[0]\n",
    "    gd = xdata[1]\n",
    "    dd = xdata[2]\n",
    "    dw = xdata[3]\n",
    "    paychar = xdata[4]\n",
    "    dc = xdata[5]\n",
    "    \n",
    "    check1= max(k/k_scaler_power, 1e-115)\n",
    "    check2= np.maximum(s/s_scaler_power + gift*0.4*gd + (beta**dd)*(delta**dw)*pay100 + alpha*paychar + a*0.01*dc, 1e-10)  \n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    sse = np.sum((f_x-dt.loc[dt['samplenw']==1].logbuttonpresses_nearest_100)**2) \n",
    "    \n",
    "    return sse\n",
    "\n",
    "nwest_aut = [20.51815, 5.17e-13, 3.26977, 0.0064462, 0.1818249, 0.0000204, 1.357934, 0.7494928] # authors'estimates\n",
    "sse_our = round(noweight_sse(args,*bp53),3)\n",
    "sse_aut = round(noweight_sse(args,*nwest_aut),3)\n",
    "\n",
    "print('The sum of squared errors using our estimates is: ' + str(sse_our))\n",
    "print(\"The sum of squared errors using the authors'estimates is: \" + str(sse_aut))\n",
    "print('The small difference between the Stata sse and the sse computed by us are most likely due to rounding.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we replicate the estimates from Panel A in Table 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sample used for Table 6 panel A\n",
    "    \n",
    "dt['t6.1']= (dt['treatment']=='6.1').astype(int)\n",
    "dt['t6.2']= (dt['treatment']=='6.2').astype(int)\n",
    "dt['samplepr']= dt['dummy1']+dt['t6.1']+dt['t6.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define f(x,θ) for the exponential cost function. Here we assume curvature of utility over piece rate = 1, (Column 4)\n",
    "# wd is the weight_dummy\n",
    "# prob is the prob_dummy\n",
    "# g, k and s are the same parameters as before\n",
    "# p_weight is the probability weighting coefficient under the assumption of linear value function in this case \n",
    "# curv is the curvature of the value function. Here curv = 1\n",
    "\n",
    "def probweight4Exp(xdata, g, k, s, p_weight):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1 = k/k_scaler_exp\n",
    "    check2 = s/s_scaler_exp + p_weight**wd*prob*pay100\n",
    "    \n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solutions for column 4 exponential cost function\n",
    "\n",
    "prob_weight_init = [0.2]\n",
    "st_valuesprobweight_exp = np.concatenate((st_values_exp,prob_weight_init))\n",
    "args = [dt.loc[dt['samplepr']==1].payoff_per_100, dt.loc[dt['samplepr']==1].weight_dummy, dt.loc[dt['samplepr']==1].prob]\n",
    "\n",
    "sol = opt.curve_fit(probweight4Exp,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].buttonpresses_nearest_100,\n",
    "                    st_valuesprobweight_exp)\n",
    "be64 = sol[0] \n",
    "se64 = np.sqrt(np.diagonal(sol[1])) \n",
    "\n",
    "# Define f(x,θ). Here we assume curvature of utility over piece rate = 0.88, Column (5)\n",
    "\n",
    "def probweight5Exp(xdata, g, k, s, p_weight):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1=k/k_scaler_exp\n",
    "    check2=s/s_scaler_exp + p_weight**wd*prob*pay100**0.88\n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solutions for column 5 exponential cost function\n",
    "\n",
    "sol = opt.curve_fit(probweight5Exp,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].buttonpresses_nearest_100,\n",
    "                    st_valuesprobweight_exp)\n",
    "be65 = sol[0]\n",
    "se65 = np.sqrt(np.diagonal(sol[1])) \n",
    "\n",
    "# Define f(x,θ). Here we we also estimate the curvature of utility over piece rate, Column (6)\n",
    "\n",
    "def probweight6Exp(xdata, g, k, s, p_weight, curv):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1=k/k_scaler_exp\n",
    "    check2=s/s_scaler_exp + p_weight**wd*prob*pay100**curv\n",
    "    \n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "# Find the solutions for column 6 exponential cost function\n",
    "\n",
    "curv_init = [0.5]\n",
    "st_valuesprobweight6_exp = np.concatenate((st_valuesprobweight_exp,curv_init))\n",
    "\n",
    "sol = opt.curve_fit(probweight6Exp,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].buttonpresses_nearest_100,\n",
    "                    st_valuesprobweight6_exp)\n",
    "be66 = sol[0]\n",
    "se66 = np.sqrt(np.diagonal(sol[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same for the power cost function specification\n",
    "\n",
    "# column 4\n",
    "\n",
    "def probweight4Power(xdata, g, k, s, p_weight):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1 = max(k/k_scaler_power, 1e-115)\n",
    "    check2 = np.maximum(s/s_scaler_power + p_weight**wd*prob*pay100, 1e-10)\n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "st_valuesprobweight_power = np.concatenate((st_values_power,prob_weight_init))\n",
    "\n",
    "sol = opt.curve_fit(probweight4Power,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].logbuttonpresses_nearest_100,\n",
    "                    st_valuesprobweight_power)\n",
    "bp61 = sol[0]\n",
    "sp61 = np.sqrt(np.diagonal(sol[1]))\n",
    "\n",
    "# column 5\n",
    "\n",
    "def probweight5Power(xdata, g, k, s, p_weight):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1 = max(k/k_scaler_power, 1e-115)\n",
    "    check2 = np.maximum(s/s_scaler_power+p_weight**wd*prob*pay100**0.88, 1e-10)\n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "sol = opt.curve_fit(probweight5Power,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].logbuttonpresses_nearest_100,\n",
    "                    st_valuesprobweight_power)\n",
    "bp62 = sol[0]\n",
    "sp62 = np.sqrt(np.diagonal(sol[1])) \n",
    "\n",
    "# column 6\n",
    "\n",
    "def probweight6Power(xdata, g, k, s, p_weight, curv):\n",
    "    \n",
    "    pay100 = xdata[0]\n",
    "    wd = xdata[1]\n",
    "    prob = xdata[2]\n",
    "    \n",
    "    check1 = max(k/k_scaler_power, 1e-115)\n",
    "    check2 = np.maximum(s/s_scaler_power+p_weight**wd*prob*pay100**curv, 1e-10)\n",
    "    f_x = (-1/g * np.log(check1) + 1/g*np.log(check2))\n",
    "    \n",
    "    return f_x\n",
    "\n",
    "st_valuesprobweight6_power = np.concatenate((st_valuesprobweight_power,curv_init))\n",
    "\n",
    "sol = opt.curve_fit(probweight6Power,\n",
    "                    args,\n",
    "                    dt.loc[dt['samplepr']==1].logbuttonpresses_nearest_100,\n",
    "                    st_valuesprobweight6_power)\n",
    "bp63 = sol[0]\n",
    "sp63 = np.sqrt(np.diagonal(sol[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe relative to table 6 and save it as a csv file\n",
    "\n",
    "# To create arrays of the same length\n",
    "bp61 = np.append(bp61,1)\n",
    "sp61 = np.append(sp61,0)\n",
    "bp62 = np.append(bp62,0.88)\n",
    "sp62 = np.append(sp62,0)\n",
    "be64 = np.append(be64,1)\n",
    "se64 = np.append(se64,0)\n",
    "be65 = np.append(be65,0.88)\n",
    "se65 = np.append(se65,0)\n",
    "\n",
    "pnames = [\"Curvature γ of cost function\", \"Level k of cost of effort\", \"Intrinsic motivation s\", \"Probability weighting π (1%) (in %)\",\n",
    "          \"Curvature of utility over piece rate\"]\n",
    "\n",
    "t6 = pd.DataFrame({'parameters':pnames,'p_est1':bp61,'p_se1':sp61,'p_est2':bp62,'p_se2':sp62,\n",
    "                   'p_est3':bp63,'p_se3':sp63,\n",
    "                   'e_est4':be64,'e_se4':se64,'e_est5':be65,'e_se5':se65,'e_est6':be66,\n",
    "                   'e_se6':se66})\n",
    "\n",
    "t6.to_csv('../output/table6_python.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 6: Estimate of model on effort in three benchmark treatments and two probability treatments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>p_est1</th>\n",
       "      <th>p_se1</th>\n",
       "      <th>p_est2</th>\n",
       "      <th>p_se2</th>\n",
       "      <th>p_est3</th>\n",
       "      <th>p_se3</th>\n",
       "      <th>e_est4</th>\n",
       "      <th>e_se4</th>\n",
       "      <th>e_est5</th>\n",
       "      <th>e_se5</th>\n",
       "      <th>e_est6</th>\n",
       "      <th>e_se6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Curvature γ of cost function</td>\n",
       "      <td>20.95</td>\n",
       "      <td>5.78</td>\n",
       "      <td>18.96</td>\n",
       "      <td>5.27</td>\n",
       "      <td>19.64</td>\n",
       "      <td>17.32</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Level k of cost of effort</td>\n",
       "      <td>3.89e-71</td>\n",
       "      <td>1.70e-69</td>\n",
       "      <td>1.96e-64</td>\n",
       "      <td>7.68e-63</td>\n",
       "      <td>1.01e-66</td>\n",
       "      <td>1.36e-64</td>\n",
       "      <td>2.42e-14</td>\n",
       "      <td>1.29e-13</td>\n",
       "      <td>7.50e-13</td>\n",
       "      <td>3.56e-12</td>\n",
       "      <td>5.46e-8</td>\n",
       "      <td>3.70e-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intrinsic motivation s</td>\n",
       "      <td>1.57e-6</td>\n",
       "      <td>4.23e-6</td>\n",
       "      <td>5.96e-6</td>\n",
       "      <td>1.47e-5</td>\n",
       "      <td>3.75e-6</td>\n",
       "      <td>4.17e-5</td>\n",
       "      <td>1.64e-5</td>\n",
       "      <td>2.40e-5</td>\n",
       "      <td>5.55e-5</td>\n",
       "      <td>7.20e-5</td>\n",
       "      <td>3.14e-3</td>\n",
       "      <td>7.50e-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probability weighting π (1%) (in %)</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Curvature of utility over piece rate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             parameters    p_est1     p_se1    p_est2  \\\n",
       "0          Curvature γ of cost function     20.95      5.78     18.96   \n",
       "1             Level k of cost of effort  3.89e-71  1.70e-69  1.96e-64   \n",
       "2                Intrinsic motivation s   1.57e-6   4.23e-6   5.96e-6   \n",
       "3   Probability weighting π (1%) (in %)      0.19      0.17      0.37   \n",
       "4  Curvature of utility over piece rate       1.0       0.0      0.88   \n",
       "\n",
       "      p_se2    p_est3     p_se3    e_est4     e_se4    e_est5     e_se5  \\\n",
       "0      5.27     19.64     17.32    0.0134    0.0026    0.0119    0.0023   \n",
       "1  7.68e-63  1.01e-66  1.36e-64  2.42e-14  1.29e-13  7.50e-13  3.56e-12   \n",
       "2   1.47e-5   3.75e-6   4.17e-5   1.64e-5   2.40e-5   5.55e-5   7.20e-5   \n",
       "3       0.3       0.3      1.57      0.24      0.14      0.47      0.25   \n",
       "4       0.0      0.92      0.93       1.0       0.0      0.88       0.0   \n",
       "\n",
       "    e_est6    e_se6  \n",
       "0   0.0072   0.0029  \n",
       "1  5.46e-8  3.70e-7  \n",
       "2  3.14e-3  7.50e-3  \n",
       "3      4.3     5.46  \n",
       "4     0.47     0.24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of observations: 2,787\n"
     ]
    }
   ],
   "source": [
    "# Print table 6\n",
    "\n",
    "# Formatting nicely the results:\n",
    "\n",
    "columns = [bp61, sp61, bp62, sp62, bp63, sp63]\n",
    "vs = []\n",
    "for col in columns:\n",
    "    col = [round(col[0],2), '{0:.2e}'.format(Decimal(col[1]/1e+57)), '{0:.2e}'.format(Decimal(col[2]/1e+6)),\n",
    "           round(col[3],2), round(col[4],2)]\n",
    "    vs.append(col)\n",
    "\n",
    "columns = [be64, se64, be65, se65, be66, se66]\n",
    "for col in columns:\n",
    "    col = [round(col[0],4), '{0:.2e}'.format(Decimal(col[1]/1e+16)), '{0:.2e}'.format(Decimal(col[2]/1e+6)),\n",
    "           round(col[3],2), round(col[4],2)]\n",
    "    vs.append(col)\n",
    "    \n",
    "t6 = pd.DataFrame({'parameters':pnames,'p_est1':vs[0],'p_se1':vs[1],'p_est2':vs[2],'p_se2':vs[3],'p_est3':vs[4],'p_se3':vs[5],\n",
    "                   'e_est4':vs[6],'e_se4':vs[7],'e_est5':vs[8],'e_se5':vs[9],'e_est6':vs[10], 'e_se6':vs[11]})\n",
    "\n",
    "# There are some differences in the standard errors since we leave here non robust standard errors provided by curve_fit.\n",
    "# Point estimates for the power cost function are again a little different from the authors', while they are the same for the \n",
    "# exponential cost function\n",
    "\n",
    "print('Table 6: Estimate of model on effort in three benchmark treatments and two probability treatments')\n",
    "display(t6)\n",
    "print('Nr. of observations: ' + str('{0:,}'.format(len(dt.loc[dt['samplepr']==1].logbuttonpresses_nearest_100))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
